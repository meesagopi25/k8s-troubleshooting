Below is your complete Markdown / README-style training guide for Kubernetes Pending Pod Troubleshooting.
You can copy/paste this directly into a README.md file and use it for team training or workshops.

üß™ Kubernetes Troubleshooting Lab
Understanding & Fixing Pods Stuck in Pending State
This lab provides hands-on, practical scenarios covering every major reason a Kubernetes Pod may be stuck in the Pending phase.
Each scenario includes:
	‚Ä¢ What the issue is
	‚Ä¢ How to reproduce it
	‚Ä¢ How to troubleshoot it
	‚Ä¢ How to fix it
Works on:
	‚Ä¢ Minikube
	‚Ä¢ KIND
	‚Ä¢ OpenShift Local
	‚Ä¢ OpenShift Sandbox
	‚Ä¢ Any Kubernetes cluster

üß© Prerequisites
Ensure you can run:
kubectl get nodes
kubectl apply -f file.yaml
kubectl describe pod <pod>
kubectl describe node <node>

üìò Table of Contents
	1. Insufficient CPU
	2. Insufficient Memory
	3. Node Taints
	4. Unbound PVC
	5. Node Selector Mismatch
	6. Anti-Affinity Rules
	7. SCC / PSP Security Restrictions
	8. ResourceQuota Violations
	9. GPU Requests Not Allowed
	10. Ephemeral Storage Violations
	11. Common Troubleshooting Commands

1Ô∏è‚É£ Insufficient CPU
‚ùå Problem
Pod requests more CPU than available in the cluster.
YAML:
apiVersion: v1
kind: Pod
metadata:
  name: cpu-hog
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        cpu: "4000m"
      limits:
        cpu: "20"

Create the pod:
bash-5.1 ~ $ oc apply -f cpu-hog.yml 
Expected error:
Error from server (Forbidden): error when creating "cpu-hog.yml": pods "cpu-hog" is forbidden: exceeded quota: compute-deploy, requested: requests.cpu=4, used: requests.cpu=200m, limited: requests.cpu=3
Fix:
Lower CPU request.

2Ô∏è‚É£ Insufficient Memory
YAML:
apiVersion: v1
kind: Pod
metadata:
  name: memory-hog
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "30Gi"
Create the pod:
bash-5.1 ~ $ oc apply -f cpu-hog.yml 
Expected error:
bash-5.1 ~ $ oc apply -f memory-hog.yml 
The Pod "memory-hog" is invalid: spec.containers[0].resources.requests: Invalid value: "80Gi": must be less than or equal to memory limit of 1000Mi
bash-5.1 ~ $ 
Troubleshooting:
bash-5.1 ~ $ oc get limits -n mg1982-dev
NAME              CREATED AT
resource-limits   2025-11-16T06:42:07Z
bash-5.1 ~ $ oc describe limits resource-limits
Name:       resource-limits
Namespace:  mg1982-dev
Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---  ---  ---------------  -------------  -----------------------
Container   cpu       -    -    10m              1              -
Container   memory    -    -    64Mi             1000Mi         -

Perfect ‚Äî this explains exactly why your Pod failed.
Here is what your LimitRange says:
Container Memory:
  Default Request: 64Mi
  Default Limit: 1000Mi (1Gi)
  Max: none
Even though it shows ‚ÄúMax: -‚Äù, the default limit (1000Mi) becomes the effective limit unless you explicitly set your own.
And requests must always be ‚â§ limits.
Your pod was:
requests:
  memory: 30Gi
limits:
  memory: 30Gi
This violates the rule:
request.memory (30Gi) > default limit.memory (1000Mi)
So Kubernetes rejected it with:
Invalid value: "30Gi": must be less than or equal to memory limit of 1000Mi
‚úî This error is NOT from ResourceQuota
‚úî It is from LimitRange in your namespace

‚≠ê Why the request fails even though ‚ÄúMax‚Äù is empty?
Your LimitRange:
Default Request: 64Mi
Default Limit: 1000Mi
Max: -
This means:
	‚Ä¢ If you do not define a limit, Kubernetes automatically assigns 1000Mi.
	‚Ä¢ Then it checks ‚Üí
request ‚â§ limit
But your request is 30Gi, which is > 1000Mi ‚Üí invalid.
	‚Ä¢ Even if you set your own limit, if it exceeds the ResourceQuota or policy, it still may fail.

‚≠ê IMPORTANT RULE
Every container must satisfy:
request.memory ‚â§ limit.memory

Your request: 30Gi
Default limit: 1000Mi
‚Üí Violation, so pod is rejected.

‚úÖ How to fix the issue
You have two options:

OPTION 1 ‚Äî Set your own limit that is ‚â• request
Example (valid):
resources:
  requests:
    memory: 30Gi
  limits:
    memory: 30Gi
But this will still fail because of ResourceQuota?
NO ‚Äî your quota allows up to 30Gi.
It fails because you must override the default limit by explicitly defining your own.
But!
‚ö† In OpenShift Developer Sandbox, LimitRange usually enforces a maximum pod size
Even though Max is ‚Äú-‚Äù, OpenShift enforces a cluster policy that limits Pods to 1Gi.
So even if you set:
limits:
  memory: 30Gi
OpenShift Sandbox will still block it at admission because Sandbox cluster is intentionally small.

OPTION 2 ‚Äî Use a smaller request (recommended)
Example of a Pod that will succeed:
resources:
  requests:
    memory: 512Mi
  limits:
    memory: 1Gi

üéØ Conclusion
Your error happens because of this rule:
‚úî Request must be ‚â§ Limit
‚úî LimitRange sets default memory limit = 1000Mi
‚úî Your request = 30Gi > 1000Mi ‚Üí INVALID
Even though your ResourceQuota allows 30Gi,
your LimitRange restricts per-container memory to max 1000Mi
(because the cluster enforces a default limit and does not allow raising it).

üí¨ If you want:
I can explain:
‚úÖ The difference between ResourceQuota vs LimitRange
‚úÖ Why OpenShift Sandbox uses 1Gi limit
‚úÖ How to redesign Pods to fit Sandbox restrictions
Just tell me!

3Ô∏è‚É£ Node Taints
Add taint:
kubectl taint nodes <node> dedicated=backend:NoSchedule
Pod that fails:
apiVersion: v1
kind: Pod
metadata:
  name: fail-on-taint
spec:
  containers:
  - name: app
    image: nginx

Troubleshoot:
kubectl describe pod no-toleration
Expected:
node(s) had taint {dedicated=backend:NoSchedule}, that the pod didn't tolerate
Fix:
Add toleration:
tolerations:
- key: "dedicated"
  operator: "Equal"
  value: "backend"
  effect: "NoSchedule"

apiVersion: v1
kind: Pod
metadata:
  name: tolerate-backend-taint
  labels:
    app: test-app
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "backend"
    effect: "NoSchedule"

  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "200m"
        memory: "256Mi"
Great ‚Äî here are full examples of Pods that will FORCE scheduling onto a tainted node using:
	‚Ä¢ Tolerations
	‚Ä¢ NodeSelector
	‚Ä¢ NodeAffinity
These examples assume your taint is:
dedicated=backend:NoSchedule
and the target node is:
kafka-cluster-worker2
Before running these examples, ensure the node is tainted:
oc adm taint nodes kafka-cluster-worker2 dedicated=backend:NoSchedule

‚≠ê Pod that FORCES scheduling onto the tainted node using nodeSelector + toleration
‚úî Will schedule ONLY on kafka-cluster-worker2
‚úî Will FAIL if the node is down
‚úî Toleration allows bypassing the taint
Save as pod-force-nodeselector.yaml
apiVersion: v1
kind: Pod
metadata:
  name: force-nodeselector
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "backend"
    effect: "NoSchedule"
nodeSelector:
    kubernetes.io/hostname: kafka-cluster-worker2
containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
üëâ Steps:
	1. Label the node (hostname is usually auto-set):
oc label node kafka-cluster-worker2 kubernetes.io/hostname=kafka-cluster-worker2 --overwrite
	1. Deploy the pod:
oc apply -f pod-force-nodeselector.yaml
	1. Check where it landed:
oc get pod -o wide
You will see:
force-nodeselector  Running  kafka-cluster-worker2

‚≠ê Pod that FORCES scheduling using NodeAffinity + toleration
‚úî More flexible than nodeSelector
‚úî Allows matching multiple values
‚úî Still forces it onto worker2
Save as pod-force-affinity.yaml
apiVersion: v1
kind: Pod
metadata:
  name: force-affinity
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "backend"
    effect: "NoSchedule"
affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - kafka-cluster-worker2
containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
üëâ Deploy:
oc apply -f pod-force-affinity.yaml

‚≠êPod that FAILS even with NodeSelector (no toleration)
This is useful for testing Pending state.
Save as pod-fail-no-toleration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fail-no-toleration
spec:
  nodeSelector:
    kubernetes.io/hostname: kafka-cluster-worker2
containers:
  - name: nginx
    image: nginx
üëâ Result:
oc describe pod fail-no-toleration
Expected:
node(s) had taint {dedicated=backend:NoSchedule} that the pod didn't tolerate

‚≠ê Pod that FAILS with Affinity (no toleration)
Save as pod-fail-affinity.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fail-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - kafka-cluster-worker2
containers:
  - name: nginx
    image: nginx
üëâ Result:
oc describe pod fail-affinity
Expected:
node(s) had taint {dedicated=backend:NoSchedule} that the pod didn't tolerate

‚úî Summary Comparison
Pod Type	Has Toleration?	Has NodeSelection?	Expected Result
force-nodeselector	Yes	Yes	Schedules on worker2
force-affinity	Yes	Yes (via affinity)	Schedules on worker2
fail-no-toleration	No	Yes	Fails (Pending)
fail-affinity	No	Yes	Fails (Pending)

--------------------------------------------------------------------------------------------------------------------------------
4Ô∏è‚É£ Unbound PVC
Unbindable PVC:
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: bad-pvc
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 10Gi
  storageClassName: does-not-exist
Pod that fails:
apiVersion: v1
kind: Pod
metadata:
  name: pvc-pod
spec:
  containers:
  - name: busy
    image: busybox
    command: ["sleep", "3600"]
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: bad-pvc
Troubleshoot:
persistentvolumeclaim "bad-pvc" is not bound
Fix:
Use correct StorageClass or create matching PV.

5Ô∏è‚É£ Node Selector Mismatch
Failing Pod:
apiVersion: v1
kind: Pod
metadata:
  name: node-selector-fail
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: app
    image: nginx
Describe:
0 nodes match pod's node selector
Fix:
Label a node:
kubectl label node <node> disktype=ssd

6Ô∏è‚É£ Anti-Affinity Rules
Pod with impossible anti-affinity:
apiVersion: v1
kind: Pod
metadata:
  name: anti-affinity-fail
  labels:
    app: test
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: test
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: app
    image: nginx
Describe:
pod anti-affinity rules not satisfied
Fix:
Use preferred anti-affinity or reduce strictness.

Few more examples:
Node Affinity and Pod Anti-Affinity are scheduling rules Kubernetes uses to decide which nodes a Pod should or should not run on.
They give you fine-grained control over Pod placement, similar to advanced ‚ÄúnodeSelector‚Äù, but much more powerful.

üß©6. 1Node Affinity (Pod prefers/needs specific nodes)
Node Affinity tells Kubernetes:
‚úî Which nodes a Pod should run on
‚úî Or which nodes a Pod must run on**
‚úî Based on node labels (ex: environment=prod, region=us-east, instance=big)
üéØ Types of Node Affinity
Type	Meaning
requiredDuringSchedulingIgnoredDuringExecution	Pod must be scheduled on matching nodes (hard rule).
preferredDuringSchedulingIgnoredDuringExecution	Pod prefers matching nodes but will run elsewhere (soft rule).

‚≠ê Node Affinity Example ‚Äì Pod MUST run on nodes labeled env=prod
Step 1 ‚Äî Label the node:
oc label node kafka-cluster-worker env=prod
Step 2 ‚Äî Pod with Node Affinity:
apiVersion: v1
kind: Pod
metadata:
  name: pod-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: env
            operator: In
            values:
            - prod
containers:
  - name: nginx
    image: nginx
Result:
‚úî Pod runs only on nodes labeled env=prod
‚ùå If no node has that label ‚Üí Pod stays Pending
Describe output:
0/3 nodes are available: 3 node(s) didn't match node affinity

üß© 6.2 Pod Anti-Affinity (Pod must avoid certain pods)
Pod Anti-Affinity tells Kubernetes:
‚úî Do NOT place this Pod on a node that already runs similar Pods
‚úî Useful for high availability, spreading Pods across nodes
Example use cases:
	‚Ä¢ Spread replicas across nodes (avoid single-node failure)
	‚Ä¢ Avoid running two database pods on same node
	‚Ä¢ Ensure web pods do not run where cache pods run

‚≠ê Anti-Affinity Example ‚Äì Different Pods cannot share same node
Scenario
You want no two Pods with label app=web to run on the same node.
Deployment example:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: web
            topologyKey: "kubernetes.io/hostname"
containers:
      - name: nginx
        image: nginx
Result:
‚úî Each replica gets scheduled on a different node
‚úî Ensures high availability
‚úî If cluster has fewer nodes than replicas ‚Üí extra pods stay Pending
describe shows:
0/3 nodes are available: pod anti-affinity rules not satisfied

üß† How topologyKey works
topologyKey: kubernetes.io/hostname
‚Üí Spread across nodes
Other options:
	‚Ä¢ topology.kubernetes.io/zone
	‚Ä¢ topology.kubernetes.io/region
Can force pods to schedule in different zones or regions.

‚≠ê Real-World Use Cases
‚úî Node Affinity
	‚Ä¢ Run GPU workloads on nodes with GPUs
	‚Ä¢ Run high-memory workloads on special nodes
	‚Ä¢ Run production pods only on prod-labeled nodes
‚úî Pod Anti-Affinity
	‚Ä¢ Spread replicas across nodes
	‚Ä¢ Prevent two database pods from being co-located
	‚Ä¢ Ensure Redis master and slave are on different nodes

üî• Full Example Combining Both
apiVersion: v1
kind: Pod
metadata:
  name: complex-scheduler
  labels:
    app: payment
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-type
            operator: In
            values:
            - high-memory
podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: payment
          topologyKey: kubernetes.io/hostname
containers:
  - name: payment-service
    image: nginx
Result:
	‚Ä¢ Pod must run on nodes labeled node-type=high-memory
	‚Ä¢ Pod prefers not to run on nodes already running other payment pods

üéâ Summary Table
Feature	Purpose	Example
Node Affinity	Pod MUST/PREFERS to run on specific nodes	env=prod
Pod Anti-Affinity	Pod MUST/PREFERS to avoid nodes with certain pods	Spread replicas
Taints/Tolerations	Nodes repel pods; tolerations allow exceptions	maintenance nodes

If you want, I can also provide:
‚úÖ Real interview questions on Affinity & Taints
‚úÖ A combined Affinity + Taint scheduling lab
‚úÖ A cheat sheet diagram for Affinity rules
Just tell me!

7Ô∏è‚É£ SecurityContextConstraints / PSP Failure (OpenShift)
apiVersion: v1
kind: Pod
metadata:
  name: privileged-denied
spec:
  containers:
  - name: app
    image: nginx
    securityContext:
      privileged: true
Describe:
pod is not allowed to use SecurityContextConstraints "privileged"
Fix:
oc adm policy add-scc-to-user privileged -z default

üß† Why would someone use privileged mode?
	‚Ä¢ Running storage plugins
	‚Ä¢ Running device drivers
	‚Ä¢ Running monitoring tools that need host access
	‚Ä¢ Managing the host network
	‚Ä¢ Running low-level system daemons (e.g., kube-proxy)
For regular applications (Nginx, Java, Node.js apps),
privileged mode should NEVER be used.

üéØ Summary
This Pod:
	‚Ä¢ Runs nginx
	‚Ä¢ Asks for privileged mode
	‚Ä¢ Will fail in OpenShift unless privileged SCC is granted
	‚Ä¢ Would run on vanilla Kubernetes (if no PodSecurityPolicy prevents it)

If you want, I can also explain:
‚úÖ What SCCs are and how they work
‚úÖ The difference between PodSecurityPolicy vs SCC
‚úÖ How to run containers as non-root


8Ô∏è‚É£ ResourceQuota Violations
Example: Memory request over quota.
apiVersion: v1
kind: Pod
metadata:
  name: quota-memory-fail
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        memory: "40Gi"
Describe:
exceeded quota: compute-deploy, requested: requests.memory=40Gi
Fix:
Reduce memory request.

9Ô∏è‚É£ GPU Requests Not Allowed
apiVersion: v1
kind: Pod
metadata:
  name: gpu-fail
spec:
  containers:
  - name: cuda
    image: nvidia/cuda
    resources:
      requests:
        nvidia.com/gpu: 1
Error:
exceeded quota: requests.nvidia.com/gpu=1, limited: 0
Fix:
Remove GPU request.

üîü Ephemeral Storage Violations
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-fail
spec:
  containers:
  - name: app
    image: nginx
    resources:
      requests:
        ephemeral-storage: "20Gi"   # Limit is usually smaller
Describe:
exceeded quota: requests.ephemeral-storage

1Ô∏è‚É£1Ô∏è‚É£ Common Troubleshooting Commands
Check events:
kubectl describe pod <pod>
Check node conditions:
kubectl describe node <node>
Check taints:
kubectl describe node <node> | grep -i taint
Check resource usage:
kubectl top pods
kubectl top nodes
Check PVC:
kubectl get pvc
kubectl describe pvc <pvc>

üéì Conclusion
This lab provides hands-on examples and troubleshooting practice for all major causes of Pods stuck in Pending:
	‚Ä¢ Resource shortages
	‚Ä¢ Taints
	‚Ä¢ PVC issues
	‚Ä¢ Node selector mismatch
	‚Ä¢ Affinity constraints
	‚Ä¢ Security restrictions
	‚Ä¢ ResourceQuota enforcement
	‚Ä¢ GPU & ephemeral storage limits
<img width="852" height="20279" alt="image" src="https://github.com/user-attachments/assets/02752e84-5de4-4551-8822-70231e61521a" />

